# ollama-docker

## Usage 
1. Execute
```bash
docker compose up
```
2. Wait for the model setup (llama3 by default)
3. Head to web ui (http://localhost:8080 by default)

## FYI
- Repository is in progress. Only the basics have been implemented.
- For now, it is CPU only. Ollama docker image might be configured to use GPU, but it is not manageable in this repository yet.  
